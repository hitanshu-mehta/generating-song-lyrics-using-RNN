{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('songdata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57650"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 57650 songs in our"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "643"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['artist'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have songs from 643 artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Donna Summer        191\n",
       "Gordon Lightfoot    189\n",
       "Bob Dylan           188\n",
       "George Strait       188\n",
       "Loretta Lynn        187\n",
       "Alabama             187\n",
       "Cher                187\n",
       "Reba Mcentire       187\n",
       "Chaka Khan          186\n",
       "Dean Martin         186\n",
       "Name: artist, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of songs of top 10 artist is shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89.65785381026438"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['artist'].value_counts().values.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 89 songs on average for each artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ','.join(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Song lyrics is in the column 'text', so we combine all the rows of that column and store it in the variable 'data'. NOTE: separator is ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at her face, it's a wonderful face  \\nAnd it means something special to me  \\nLook at the way that she smiles when she sees me  \\nHow lucky can one fellow be?  \\n  \\nShe's just my kind of girl, she makes me feel fine  \\nWho could ever believe that she could be mine?  \\nShe's just my kind of girl, with\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are making char level RNN. so we store all the unique characters of the dataset in the variable named 'char'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since neural network only accepts inputs in number.We need to conver all the characters in vocabulary to numbers.\n",
    "We map all the characters to thier vacabulary that forms a unique number.\n",
    "We define char_to_id dictionary that maps all the character with thier index. To get character from the index we define another dictionary id_to_char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_id = {char : i for i,char in enumerate(chars)}\n",
    "id_to_char = {i:char for i,char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocab_size)[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one_hot_encoder function np.eye(vacab_size) creates a matrix of size (vocab_size X vacab_size). And we are return one row according the index from that matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of units in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the length of input and output sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the learning rate of the gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "tf.set_random_seed(seed_value)\n",
    "random.seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the seed value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(shape = [None,vocab_size],dtype = tf.float32,name = \"inputs\")\n",
    "targets = tf.placeholder(shape = [None,vocab_size],dtype = tf.float32,name = \"targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the placeholder for input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.placeholder(shape = [1,hidden_size],dtype = tf.float32,name = \"state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the placeholder for initial hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.random_normal_initializer(stddev = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the initializer for initializing weights of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining forward propogation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"RNN\") as scope:\n",
    "    h_t = init_state\n",
    "    y_hat = []\n",
    "    \n",
    "    for t, x_t in enumerate(tf.split(inputs,seq_length,axis = 0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "        U  = tf.get_variable(\"U\",[vocab_size,hidden_size],initializer = initializer)\n",
    "\n",
    "        W = tf.get_variable(\"W\",[hidden_size,hidden_size],initializer = initializer)\n",
    "\n",
    "        V = tf.get_variable(\"V\",[hidden_size,vocab_size],initializer = initializer)\n",
    "\n",
    "        bh = tf.get_variable(\"bh\",[hidden_size],initializer = initializer)\n",
    "\n",
    "        by = tf.get_variable(\"by\",[vocab_size],initializer = initializer)\n",
    "\n",
    "        h_t = tf.tanh(tf.matmul(x_t,U) + tf.matmul(h_t,W) + bh)\n",
    "\n",
    "        y_hat_t = tf.matmul(h_t,V) + by\n",
    "\n",
    "        y_hat.append(y_hat_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply softmax on the output and get probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_softmax = tf.nn.softmax(y_hat[-1])\n",
    "outputs = tf.concat(y_hat,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = targets,logits = outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the final hidden state of RNN in hprev. We use this final hidden state for making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_prev = h_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining BPTT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradients of the loss with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = minimizer.compute_gradients(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the threshold for the gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = tf.constant(5.0,name = \"gradient_clipping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip the which exceeds the threshold and bring it to the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_gradients = []\n",
    "for grad,var in gradients:\n",
    "    clipped_grad = tf.clip_by_value(grad,-threshold,threshold)\n",
    "    clipped_gradients.append((clipped_grad,var))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the gradients with the clipped gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_gradients = minimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start generating song lyrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the Tensorflow session and initialize all the varibables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete code block for generating songs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = 0\n",
    "iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " After 0 iterations\n",
      "\n",
      " ht a can't na proue  \n",
      "Lifter \n",
      "\n",
      "Ty worlans  \n",
      "I feel feeling upsa I soul ngallys  \n",
      "You goone bisk wor gidl you, I could suse craichur oh-ne turns rinex  \n",
      "Way do me sithing not crick  \n",
      "  \n",
      "As kist  \n",
      "  \n",
      "I  \n",
      "Then I ween's so good  \n",
      "Onua It's do ba me gind you alling you up aind I will say: my hows  \n",
      "Of is non't need,  \n",
      "  \n",
      "[ChorKs]  \n",
      "I wake me stanns sire that  \n",
      "I woulds, world tolone take your fad you worllast.  \n",
      "Sted my mind alone  \n",
      "Can I willow time,  \n",
      "And no me ar the ur a mace to go  \n",
      "And in to 'm \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if iteration == 50000:\n",
    "        break\n",
    "    \n",
    "    if pointer + seq_length+1 >= len(data) or iteration == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        pointer = 0  \n",
    "    \n",
    "    # select input sentence\n",
    "    input_sentence = data[pointer:pointer + seq_length]\n",
    "    \n",
    "    # select output sentence \n",
    "    output_sentence = data[pointer + 1: pointer + seq_length + 1]\n",
    "    \n",
    "    # get the indices of input and output sentences\n",
    "    input_indices = [char_to_id[ch] for ch in input_sentence]\n",
    "    target_indices = [char_to_id[ch] for ch in output_sentence]\n",
    "    \n",
    "    # convert the indices to one-hot encoded vectors with the help of their indices\n",
    "    input_vectors = one_hot_encoder(input_indices)\n",
    "    target_vectors = one_hot_encoder(target_indices)\n",
    "    \n",
    "    # train the network and get the final output state\n",
    "    hprev_val,loss_val,_ = sess.run([h_prev,loss,updated_gradients],feed_dict = {inputs: input_vectors,targets: target_vectors,init_state: hprev_val})\n",
    "    \n",
    "    # make prediction on every 500th iteration\n",
    "    \n",
    "    if iteration % 500 == 0:\n",
    "        \n",
    "        #length of characters we want to predict\n",
    "        sample_length = 500\n",
    "        \n",
    "        #randomly select index \n",
    "        random_index = random.randint(0,len(data) - seq_length)\n",
    "        \n",
    "        #sample the input sentence with randomly selected index \n",
    "        sample_input_sent = data[random_index : random_index + seq_length]\n",
    "        \n",
    "        #get indices of the sampled input sentences \n",
    "        sample_input_indices = [char_to_id[ch] for ch in sample_input_sent]\n",
    "        \n",
    "        #store the previous state in sample_prev_state_val\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "        \n",
    "        #for storing indiced of prediced values\n",
    "        predicted_indices = []\n",
    "        \n",
    "        for t in range(sample_length):\n",
    "            \n",
    "            # convert the sample input indices to one-hot encoded vectors\n",
    "            sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "            \n",
    "            # compute the probability of all the words in the vocabulary to be the next character\n",
    "            probs_dict,sample_prev_state_val = sess.run([output_softmax,h_prev],feed_dict = { inputs:sample_input_vector, init_state :sample_prev_state_val})\n",
    "            \n",
    "            # we randomly select index with the probability distribution generated by our model\n",
    "            ix = np.random.choice(range(vocab_size), p = probs_dict.ravel())\n",
    "            \n",
    "            sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "            \n",
    "            #store the predicted index in predicted_indices list\n",
    "            predicted_indices.append(ix)\n",
    "            \n",
    "        #convert the predicted indices to their character\n",
    "        predicted_chars = [id_to_char[ix] for ix in predicted_indices]\n",
    "        \n",
    "        #combine the predcited characters\n",
    "        text = ''.join(predicted_chars)\n",
    "        \n",
    "        #predict the predict text on every 50000th iteration\n",
    "        if iteration %5000 == 0:           \n",
    "            print ('\\n')\n",
    "            print (' After %d iterations' %(iteration))\n",
    "            print('\\n %s \\n' % (text,))   \n",
    "            print('-'*115)\n",
    "            \n",
    "    #increment the pointer and iteration\n",
    "    pointer += seq_length\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
